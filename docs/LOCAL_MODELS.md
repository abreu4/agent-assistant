# Local Model Random Selection

## Overview

The Agent Assistant can randomly select from multiple local Ollama models for each query. This provides variety and allows you to compare different models' responses.

## Configuration

### Available Models

The following local models are configured (in `config/config.yaml`):

1. **llama3.1:8b** - Meta's Llama 3.1, balanced performance
2. **llama3.2:3b** - Smaller, faster Llama model
3. **mistral:7b** - Efficient and capable 7B model
4. **phi3:mini** - Microsoft's compact model
5. **gemma2:2b** - Google's lightweight model
6. **qwen2.5:3b** - Alibaba's multilingual model

### Enable/Disable Random Selection

Edit `config/config.yaml`:

```yaml
llm:
  local:
    random_selection: true  # Set to false to always use default model
```

**Enabled (true)**: Randomly picks a model for each query
**Disabled (false)**: Always uses `llama3.1:8b` (default)

## How It Works

### With Random Selection Enabled

1. User submits a query
2. System routes to local tier (if appropriate)
3. **Random model is selected** from available models
4. Model processes the query
5. Response shows which model was used

### Example Session

```
‚ùØ What is quantum computing?

‚†π Thinking...

üé≤ Switched to local model: phi3:mini

============================================================
ü§ñ Response (model: local (phi3:mini))
============================================================
Quantum computing is...


Generated by local (phi3:mini)
============================================================

‚ùØ Explain machine learning

‚†π Thinking...

üé≤ Switched to local model: mistral:7b

============================================================
ü§ñ Response (model: local (mistral:7b))
============================================================
Machine learning is...


Generated by local (mistral:7b)
============================================================
```

## Model Information Display

The CLI shows which model was used in two places:

1. **Header**: `ü§ñ Response (model: local (mistral:7b))`
2. **Footer**: `Generated by local (mistral:7b)`

## Installing Models

To use all configured models, install them with Ollama:

```bash
# Install all models
ollama pull llama3.1:8b
ollama pull llama3.2:3b
ollama pull mistral:7b
ollama pull phi3:mini
ollama pull gemma2:2b
ollama pull qwen2.5:3b

# Or install selectively
ollama pull llama3.1:8b
ollama pull mistral:7b
```

## Adding More Models

Edit `config/config.yaml`:

```yaml
llm:
  local:
    available_models:
      - id: llama3.1:8b
        name: Llama 3.1 8B
        description: Meta's Llama 3.1 - balanced performance
      # Add your model here
      - id: your-model:tag
        name: Your Model Name
        description: Description of your model
```

## Checking Available Models

List installed Ollama models:

```bash
ollama list
```

## Performance Considerations

**Smaller models (2B-3B)**:
- ‚úÖ Faster responses
- ‚úÖ Lower memory usage
- ‚ö†Ô∏è May be less accurate for complex tasks

**Larger models (7B-8B)**:
- ‚úÖ Better quality responses
- ‚úÖ More knowledge
- ‚ö†Ô∏è Slower
- ‚ö†Ô∏è More memory required

## Fallback Behavior

If a randomly selected model fails:
- System falls back to `mistral:7b` (configured fallback)
- If fallback also fails, query fails with error

## Disabling Random Selection

To always use the same local model:

```yaml
llm:
  local:
    random_selection: false  # Disable random selection
    model: llama3.1:8b       # Always use this model
```

## Tips

1. **First Time**: Install at least 2-3 models for variety
2. **Testing**: Enable random selection to compare models
3. **Production**: Disable if you want consistent behavior
4. **Memory**: Don't install all models if RAM is limited
5. **Speed**: Use smaller models (2B-3B) for faster responses

## Model Switching Log

When random selection is enabled, you'll see:

```
üé≤ Switched to local model: phi3:mini
```

This appears when a different model is selected than the previous query.

## Comparison

| Feature | Random ON | Random OFF |
|---------|-----------|------------|
| Model Used | Changes each query | Always same |
| Response Style | Varies | Consistent |
| Performance | Mixed | Predictable |
| Fun Factor | High üé≤ | Standard |
| Testing | Great for comparing | N/A |
